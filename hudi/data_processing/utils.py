

import pandas as pd
import os
import datetime

import pyarrow as pa
import pyarrow.parquet as pq

# Define a custom function to parse the 'dropoff_datetime' column
def custom_datetime_parser(value):
    try:
        res = datetime.datetime.strptime(value, "%m/%d/%Y %I:%M:%S %p")
        return res
    except ValueError:
        return None
    

def transform_csv_to_parquet():

# ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 
    # 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 
    # 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']

    csv_file = "/home/research/datasets/2017_Yellow_Taxi_Trip_Data.csv"

    dtype = {0: str}
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file, dtype=dtype)

    # Data Cleaning Steps:
    # 1. Remove rows with missing values in key columns (you can customize this)
    # 2. Convert relevant columns to appropriate data types
    # 3. Perform any other specific data cleaning tasks as needed

    # Example: Removing rows with missing values in key columns (adjust as needed)
    key_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'fare_amount', 'total_amount']
    df = df.dropna(subset=key_columns)

    # Example: Converting columns to appropriate data types (adjust as needed)
    # df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
    # df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])
    df['passenger_count'] = df['passenger_count'].astype(int)

    # df['tpep_pickup_datetime'] = df['tpep_pickup_datetime'].apply(custom_datetime_parser)
    # df['tpep_dropoff_datetime'] = df['tpep_dropoff_datetime'].apply(custom_datetime_parser)

    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], format='%m/%d/%Y %I:%M:%S %p')
    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], format='%m/%d/%Y %I:%M:%S %p')

    # df = df.astype({'tpep_pickup_datetime': 'datetime64[ms]'})
    # df = df.astype({'tpep_dropoff_datetime': 'datetime64[ms]'})

    # Convert datetime columns to epoch milliseconds
    df['tpep_pickup_datetime'] = (df['tpep_pickup_datetime'].astype('int64') // 10**9)
    df['tpep_dropoff_datetime'] = (df['tpep_dropoff_datetime'].astype('int64') // 10**9)

    # Drop rows where 'pickup_datetime', 'dropoff_datetime' could not be parsed
    df = df.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])
    df['record_id'] = range(1, len(df) + 1)

    # Analysis of PULocationID
    unique_pulocation_ids = df['PULocationID'].nunique()
    pulocation_id_range = (df['PULocationID'].min(), df['PULocationID'].max())

    print(f"Unique PULocationID count: {unique_pulocation_ids}")
    print(f"PULocationID value range: {pulocation_id_range}")

    # # Specify the path for the output Parquet file
    # parquet_file = '/home/pg/hudi/parquet_files/nyc.parquet'

    # # Convert the cleaned DataFrame to Parquet format
    # df.to_parquet(parquet_file, index=False)


def transform_tbl_to_parquet(scale):

    # Define the file path
    tbl_file = f'/home/research/datasets/tpc-h/tpch-dbgen/lineitem_scale_{scale}.tbl'

    # Skip the first row (header row) when reading the file
    df = pd.read_csv(tbl_file, sep='|', header=None)

    print(f"Number of records: {df.shape[0]}")

    # Define column names based on the schema
    column_names = ['l_orderkey', 'l_partkey', 'l_suppkey', 'l_linenumber', 'l_quantity', 'l_extendedprice',
                    'l_discount', 'l_tax', 'l_returnflag', 'l_linestatus', 'l_shipdate', 'l_commitdate',
                    'l_receiptdate', 'l_shipinstruct', 'l_shipmode', 'l_comment', 'l_extra']

    # Assign column names to the DataFrame
    df.columns = column_names

    # Drop the extra empty column generated by trailing separator
    df = df.drop('l_extra', axis=1)

    # Add a 'record_id' column as an auto-incrementing ID
    df['record_id'] = range(1, len(df) + 1)

    # Specify the Parquet file path
    parquet_file = f'/home/pg/hudi/parquet_files/tpc_{scale}.parquet'

    # Convert the DataFrame to Parquet format directly using pandas
    df.to_parquet(parquet_file, index=False)




if __name__ == '__main__':

    transform_csv_to_parquet()
    # for i in [1, 2, 4, 8, 16]:
    #     transform_tbl_to_parquet(i)